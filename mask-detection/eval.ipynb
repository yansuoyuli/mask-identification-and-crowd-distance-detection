{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import colorsys\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "from yolo import YOLO\n",
    "from nets.yolo4 import YoloBody\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "from utils.utils import non_max_suppression, bbox_iou, DecodeBox,letterbox_image,yolo_correct_boxes\n",
    "\n",
    "Cuda = True\n",
    "device = torch.device(\"cuda\")\n",
    "# print(cv2.getBuildInformation())\n",
    "print(cv2.cuda.getCudaEnabledDeviceCount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground-truth annotations path\n",
    "annotation_path = 'datasets/val/Annotations'\n",
    "annotations = sorted(os.listdir(annotation_path))\n",
    "# test images path\n",
    "image_path = 'datasets/val/JPEGImages'\n",
    "images = sorted(os.listdir(image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start making ground-truth!\n",
      "Finish making ground-truth!\n"
     ]
    }
   ],
   "source": [
    "# making ground-truth\n",
    "print('Start making ground-truth!')\n",
    "for annotation in annotations:\n",
    "    tree = ET.parse(os.path.join(annotation_path, annotation))\n",
    "    objects = tree.findall('object')\n",
    "    with open(os.path.join('mAP/input/ground-truth', annotation.split('.')[0] + '.txt'), 'w+') as f:\n",
    "        for obj in objects:\n",
    "            name = str(obj.find('name').text)\n",
    "            bndbox = obj.find('bndbox')\n",
    "            xmin = str(bndbox.find('xmin').text)\n",
    "            ymin = str(bndbox.find('ymin').text)\n",
    "            xmax = str(bndbox.find('xmax').text)\n",
    "            ymax = str(bndbox.find('ymax').text)\n",
    "            f.write(' '.join((name, xmin, ymin, xmax, ymax)) + '\\n')\n",
    "print('Finish making ground-truth!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start making detection results!\n",
      "Loading pretrained weights.\n",
      "Finish loading!\n",
      "model_data/yolov4_maskdetect_weights1.pth model, anchors, and classes loaded.\n",
      "(1, 4)\n",
      "[done] val_1.jpg\n",
      "(1, 4)\n",
      "[done] val_10.jpg\n",
      "(1, 4)\n",
      "[done] val_100.jpg\n",
      "(1, 4)\n",
      "[done] val_101.jpg\n",
      "(1, 4)\n",
      "[done] val_102.jpg\n",
      "(1, 4)\n",
      "[done] val_103.jpg\n",
      "(1, 4)\n",
      "[done] val_104.jpg\n",
      "(1, 4)\n",
      "[done] val_105.jpg\n",
      "(1, 4)\n",
      "[done] val_106.jpg\n",
      "(1, 4)\n",
      "[done] val_107.jpg\n",
      "(1, 4)\n",
      "[done] val_108.jpg\n",
      "(1, 4)\n",
      "[done] val_109.jpg\n",
      "(1, 4)\n",
      "[done] val_11.jpg\n",
      "(1, 4)\n",
      "[done] val_110.jpg\n",
      "(1, 4)\n",
      "[done] val_111.jpg\n",
      "(1, 4)\n",
      "[done] val_112.jpg\n",
      "(1, 4)\n",
      "[done] val_113.jpg\n",
      "[done] val_114.jpg\n",
      "(1, 4)\n",
      "[done] val_115.jpg\n",
      "(1, 4)\n",
      "[done] val_116.jpg\n",
      "(1, 4)\n",
      "[done] val_117.jpg\n",
      "(1, 4)\n",
      "[done] val_118.jpg\n",
      "(1, 4)\n",
      "[done] val_119.jpg\n",
      "(1, 4)\n",
      "[done] val_12.jpg\n",
      "(1, 4)\n",
      "[done] val_120.jpg\n",
      "(1, 4)\n",
      "[done] val_121.jpg\n"
     ]
    }
   ],
   "source": [
    "# making detection-results\n",
    "class mAP_Yolo(YOLO):\n",
    "    #---------------------------------------------------#\n",
    "    #   检测图片\n",
    "    #---------------------------------------------------#\n",
    "    def detect_image(self, image_id, image):\n",
    "        self.confidence = 0.05\n",
    "        image_shape = np.array(np.shape(image)[0:2])\n",
    "\n",
    "        crop_img = np.array(letterbox_image(image, (self.model_image_size[0], self.model_image_size[1])))\n",
    "        photo = np.array(crop_img, dtype = np.float32)\n",
    "        photo /= 255.0\n",
    "        photo = np.transpose(photo, (2, 0, 1))\n",
    "        photo = photo.astype(np.float32)\n",
    "        images = []\n",
    "        images.append(photo)\n",
    "        images = np.asarray(images)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            images = torch.from_numpy(images)\n",
    "            if self.cuda:\n",
    "                images = images.cuda()\n",
    "            outputs = self.net(images)\n",
    "            \n",
    "        output_list = []\n",
    "        for i in range(3):\n",
    "            output_list.append(self.yolo_decodes[i](outputs[i]))\n",
    "        output = torch.cat(output_list, 1)\n",
    "        batch_detections = non_max_suppression(output, len(self.class_names),\n",
    "                                                conf_thres=self.confidence,\n",
    "                                                nms_thres=0.3)\n",
    "\n",
    "        try:\n",
    "            batch_detections = batch_detections[0].cpu().numpy()\n",
    "        except:\n",
    "            return image\n",
    "            \n",
    "        top_index = batch_detections[:,4]*batch_detections[:,5] > self.confidence\n",
    "        top_conf = batch_detections[top_index,4]*batch_detections[top_index,5]\n",
    "        top_label = np.array(batch_detections[top_index,-1],np.int32)\n",
    "        top_bboxes = np.array(batch_detections[top_index,:4])\n",
    "        top_xmin, top_ymin, top_xmax, top_ymax = np.expand_dims(top_bboxes[:,0],-1),np.expand_dims(top_bboxes[:,1],-1),np.expand_dims(top_bboxes[:,2],-1),np.expand_dims(top_bboxes[:,3],-1)\n",
    "\n",
    "        # 去掉灰条\n",
    "        boxes = yolo_correct_boxes(top_ymin,top_xmin,top_ymax,top_xmax,np.array([self.model_image_size[0],self.model_image_size[1]]),image_shape)\n",
    "\n",
    "        with open(os.path.join('mAP/input/detection-results', image_id + '.txt'), 'w+') as f:\n",
    "            for i, c in enumerate(top_label):\n",
    "                predicted_class = self.class_names[c]\n",
    "                score = str(top_conf[i])\n",
    "\n",
    "                top, left, bottom, right = boxes[i]\n",
    "                f.write(' '.join((str(predicted_class), str(score), str(left), str(top), str(right), str(bottom))) + '\\n')\n",
    "    \n",
    "\n",
    "print('Start making detection results!')\n",
    "yolo = mAP_Yolo()\n",
    "for image in images:\n",
    "    img = Image.open(os.path.join(image_path, image))\n",
    "    yolo.detect_image(image.split('.')[0], img)\n",
    "    print('[done] ' + image)\n",
    "print('Finish making detection results!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
