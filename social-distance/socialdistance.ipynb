{"cells":[{"cell_type":"markdown","metadata":{"id":"BBR4rYY6eINU"},"source":["# Install Dependencies"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1663240915483,"user":{"displayName":"YJ J","userId":"15370421716120110768"},"user_tz":-480},"id":"A578sDm_-BJ9","vscode":{"languageId":"python"}},"outputs":[],"source":["# import dependencies\n","from IPython.display import display, Javascript, Image\n","from base64 import b64decode, b64encode\n","import os\n","import cv2\n","import numpy as np\n","import PIL\n","import io\n","import html\n","import time\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1663241358980,"user":{"displayName":"YJ J","userId":"15370421716120110768"},"user_tz":-480},"id":"FGNvNdcX9rGp","vscode":{"languageId":"python"}},"outputs":[],"source":["# change makefile to have GPU, OPENCV and LIBSO enabled\n","!sed -i 's/OPENCV=0/OPENCV=1/' Makefile\n","!sed -i 's/GPU=0/GPU=1/' Makefile\n","!sed -i 's/CUDNN=0/CUDNN=1/' Makefile\n","!sed -i 's/CUDNN_HALF=0/CUDNN_HALF=1/' Makefile\n","!sed -i 's/LIBSO=0/LIBSO=1/' Makefile"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":83544,"status":"ok","timestamp":1663241445888,"user":{"displayName":"YJ J","userId":"15370421716120110768"},"user_tz":-480},"id":"a5ogzikW9wk6","outputId":"5b1db45f-3794-46dd-df3f-1d2cdd5beba3","vscode":{"languageId":"python"}},"outputs":[],"source":["# make darknet (builds darknet so that you can then use the darknet.py file and have its dependencies)\n","!make"]},{"cell_type":"markdown","metadata":{"id":"F5o5Gs75ZuMv"},"source":["## Darknet for Python\n","In order to utilize YOLOv4 with Python code we will use some of the pre-built functions found within darknet.py by importing the functions into our workstation. Feel free to checkout the darknet.py file to see the function definitions in detail! "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"elapsed":738,"status":"error","timestamp":1663241486683,"user":{"displayName":"YJ J","userId":"15370421716120110768"},"user_tz":-480},"id":"JhrQqPc2HR27","outputId":"07710e0a-d565-4ba1-c0a9-34a73b5cc9fb","vscode":{"languageId":"python"}},"outputs":[],"source":["# import darknet functions to perform object detections\n","from darknet import *\n","# load in our YOLOv4 architecture network\n","network, class_names, class_colors = load_network(\"cfg/yolov4.cfg\", \"cfg/coco.data\", \"DeepSocial.weights\")\n","width = network_width(network)\n","height = network_height(network)\n","\n","# darknet helper function to run detection on image\n","def darknet_helper(img, width, height):\n","  darknet_image = make_image(width, height, 3)\n","  img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","  img_resized = cv2.resize(img_rgb, (width, height),\n","                              interpolation=cv2.INTER_LINEAR)\n","\n","  # get image ratios to convert bounding boxes to proper size\n","  img_height, img_width, _ = img.shape\n","  width_ratio = img_width/width\n","  height_ratio = img_height/height\n","\n","  # run model on darknet style image to get detections\n","  copy_image_from_bytes(darknet_image, img_resized.tobytes())\n","  detections = detect_image(network, class_names, darknet_image)\n","  free_image(darknet_image)\n","  return detections, width_ratio, height_ratio"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"elapsed":676,"status":"error","timestamp":1663240895789,"user":{"displayName":"YJ J","userId":"15370421716120110768"},"user_tz":-480},"id":"TwfFDhs5TMhi","outputId":"f845ed15-8037-45bf-9124-eca9c5d00495","vscode":{"languageId":"python"}},"outputs":[],"source":["# run test on dog.jpg image that comes with repository\n","image = cv2.imread(\"data/dog.jpg\")\n","detections, width_ratio, height_ratio = darknet_helper(image, width, height)\n","\n","for label, confidence, bbox in detections:\n","  left, top, right, bottom = bbox2points(bbox)\n","  left, top, right, bottom = int(left * width_ratio), int(top * height_ratio), int(right * width_ratio), int(bottom * height_ratio)\n","  cv2.rectangle(image, (left, top), (right, bottom), class_colors[label], 2)\n","  cv2.putText(image, \"{} [{:.2f}]\".format(label, float(confidence)),\n","                    (left, top - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n","                    class_colors[label], 2)\n","cv2_imshow(image)"]},{"cell_type":"markdown","metadata":{"id":"1UwGgIrNHX43"},"source":["# SORT : Simple Object Real-time Tracking"]},{"cell_type":"markdown","metadata":{"id":"C8N1UNGR1l-j"},"source":["Copy \"sort.py\" into the Darknet folder:\n","\n","link: https://drive.google.com/file/d/1JWr5HJd7ak37CsIlXJKWFAqVue50-50q/view?usp=sharing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5826,"status":"ok","timestamp":1663241280884,"user":{"displayName":"YJ J","userId":"15370421716120110768"},"user_tz":-480},"id":"mwFwCdkkg5wA","outputId":"76f8379b-876b-4fa2-9e6a-9d86c78428bf","vscode":{"languageId":"python"}},"outputs":[],"source":["!pip install filterpy\n","from sort import *\n","mot_tracker    = Sort(max_age=25, min_hits=4, iou_threshold=0.3)"]},{"cell_type":"markdown","metadata":{"id":"nCom1unqjlYb"},"source":["# Input Setting "]},{"cell_type":"markdown","metadata":{"id":"o1RwjlrZ1u-q"},"source":["Copy inputfile into the Darknet folder:"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1663241307086,"user":{"displayName":"YJ J","userId":"15370421716120110768"},"user_tz":-480},"id":"B0KTAQFfkK3r","vscode":{"languageId":"python"}},"outputs":[],"source":["Input            = \"./XXX.avi\"\n","ReductionFactor  = 2\n","calibration      = [[180,162],[618,0],[552,540],[682,464]]"]},{"cell_type":"markdown","metadata":{"id":"c7TX9sAWew-q"},"source":["# Socialdistance"]},{"cell_type":"markdown","metadata":{"id":"6qbkxxim2FMi"},"source":["Copy \"socialdistance.py\" into the Darknet folder: \n","\n"," link ï¼š  https://drive.google.com/file/d/1nxQwnfksTGv84GnkK0usLmJI1QITN_C-/view?usp=sharing"]},{"cell_type":"markdown","metadata":{"id":"u6N3zwEPjZvu"},"source":["Configration :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i2aeq4ydcubo","vscode":{"languageId":"python"}},"outputs":[],"source":["from socialdistance import *\n","######################## Frame number\n","StartFrom  = 0 \n","EndAt      = 1500                       #-1 for the end of the video\n","\n","######################## (0:OFF/ 1:ON) Outputs\n","CouplesDetection    = 1                # Enable Couple Detection \n","DTC                 = 1                # Detection, Tracking and Couples \n","SocialDistance      = 1\n","######################## Units are Pixel\n","ViolationDistForIndivisuals = 28 \n","ViolationDistForCouples     = 31\n","####\n","CircleradiusForIndivsual    = 14\n","CircleradiusForCouples      = 17\n","######################## \n","MembershipDistForCouples    = (16 , 10) # (Forward, Behind) per Pixel\n","MembershipTimeForCouples    = 35        # Time for considering as a couple (per Frame)\n","######################## (0:OFF/ 1:ON)\n","CorrectionShift  = 1                    # Ignore people in the margins of the video\n","HumanHeightLimit = 200                  # Ignore people with unusual heights\n","########################\n","Transparency        = 0.7\n","######################## Output Video's path\n","Path_For_DTC = os.getcwd() + \"/DTC.avi\"\n","Path_For_SocialDistance = os.getcwd() + \"/Social Distancing.avi\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VjNIB3nHiL3E","vscode":{"languageId":"python"}},"outputs":[],"source":["def extract_humans(detections):\n","    detetcted = []\n","    if len(detections) > 0: # At least 1 detection in the image and check detection presence in a frame  \n","        idList = []\n","        id = 0\n","        for label, confidence, bbox in detections:\n","            if label == 'person': \n","                xmin, ymin, xmax, ymax = bbox2points(bbox)\n","                id +=1\n","                if id not in idList: idList.append(id)\n","                detetcted.append([int(xmin), int(ymin), int(xmax), int(ymax), idList[-1]])\n","    return np.array(detetcted)\n","\n","def centroid(detections, image, calibration, _centroid_dict, CorrectionShift, HumanHeightLimit):\n","    e = birds_eye(image.copy(), calibration)\n","    centroid_dict = dict()\n","    now_present = list()\n","    if len(detections) > 0:   \n","        for d in detections:\n","            p = int(d[4])\n","            now_present.append(p)\n","            xmin, ymin, xmax, ymax = d[0], d[1], d[2], d[3]\n","            w = xmax - xmin\n","            h = ymax - ymin\n","            x = xmin + w/2\n","            y = ymax - h/2\n","            if h < HumanHeightLimit:\n","                overley = e.image\n","                bird_x, bird_y = e.projection_on_bird((x, ymax))\n","                if CorrectionShift:\n","                    if checkupArea(overley, 1, 0.25, (x, ymin)):\n","                        continue\n","                e.setImage(overley)\n","                center_bird_x, center_bird_y = e.projection_on_bird((x, ymin))\n","                centroid_dict[p] = (\n","                            int(bird_x), int(bird_y),\n","                            int(x), int(ymax), \n","                            int(xmin), int(ymin), int(xmax), int(ymax),\n","                            int(center_bird_x), int(center_bird_y))\n","\n","                _centroid_dict[p] = centroid_dict[p]\n","    return _centroid_dict, centroid_dict, e.image\n","\n","def ColorGenerator(seed=1, size=10):\n","    np.random.seed = seed\n","    color=dict()\n","    for i in range(size):\n","        h = int(np.random.uniform() *255)\n","        color[i]= h\n","    return color\n","\n","def VisualiseResult(_Map, e):\n","    Map = np.uint8(_Map)\n","    histMap = e.convrt2Image(Map)\n","    visualBird = cv2.applyColorMap(np.uint8(_Map), cv2.COLORMAP_JET)\n","    visualMap = e.convrt2Image(visualBird)\n","    visualShow = cv2.addWeighted(e.original, 0.7, visualMap, 1 - 0.7, 0)\n","    return visualShow, visualBird, histMap"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":244883,"status":"error","timestamp":1662774230236,"user":{"displayName":"YJ J","userId":"15370421716120110768"},"user_tz":-480},"id":"X3BJHjILXMu8","outputId":"649f5a57-69ee-4bee-acc6-533ba4913567","vscode":{"languageId":"python"}},"outputs":[],"source":["cap = cv2.VideoCapture(Input)\n","frame_width = int(cap.get(3))\n","frame_height = int(cap.get(4))\n","height, width = frame_height // ReductionFactor, frame_width // ReductionFactor\n","print(\"Video Reolution: \",(width, height))\n","\n","if DTC: DTCVid = cv2.VideoWriter(Path_For_DTC, cv2.VideoWriter_fourcc(*\"MJPG\"), 30.0, (width, height))\n","if SocialDistance: SDimageVid = cv2.VideoWriter(Path_For_SocialDistance, cv2.VideoWriter_fourcc(*\"MJPG\"), 30.0, (width, height))\n","\n","colorPool = ColorGenerator(size = 3000)\n","_centroid_dict = dict()\n","_numberOFpeople = list()\n","_greenZone = list()\n","_redZone = list()\n","_yellowZone = list()\n","_final_redZone = list()\n","_relation = dict()\n","_couples = dict()\n","_trackMap = np.zeros((height, width, 3), dtype=np.uint8)\n","_crowdMap = np.zeros((height, width), dtype=np.int) \n","_allPeople = 0\n","_counter = 1\n","frame = 0\n","\n","while True:\n","    print('-- Frame : {}'.format(frame))\n","    prev_time = time.time()\n","    ret, frame_read = cap.read()\n","    if not ret: break\n","    frame += 1\n","    if frame <= StartFrom: continue\n","    if frame != -1:\n","        if frame > EndAt: break\n","        \n","    frame_resized = cv2.resize(frame_read,(width, height), interpolation=cv2.INTER_LINEAR)\n","    image = frame_resized\n","    e = birds_eye(image, calibration)\n","    detections, width_ratio, height_ratio = darknet_helper(image, width, height)\n","    humans = extract_humans(detections)\n","    track_bbs_ids = mot_tracker.update(humans) if len(humans) != 0 else humans\n","\n","    _centroid_dict, centroid_dict, partImage = centroid(track_bbs_ids, image, calibration, _centroid_dict, CorrectionShift, HumanHeightLimit)\n","    redZone, greenZone = find_zone(centroid_dict, _greenZone, _redZone, criteria=ViolationDistForIndivisuals)\n","    \n","    if CouplesDetection:\n","        _relation, relation = find_relation(e, centroid_dict, MembershipDistForCouples, redZone, _couples, _relation)\n","        _couples, couples, coupleZone = find_couples(image, _centroid_dict, relation, MembershipTimeForCouples, _couples)\n","        yellowZone, final_redZone, redGroups = find_redGroups(image, centroid_dict, calibration, ViolationDistForCouples, redZone, coupleZone, couples , _yellowZone, _final_redZone)\n","    else:\n","        couples = []\n","        coupleZone = []\n","        yellowZone = []\n","        redGroups = redZone\n","        final_redZone = redZone\n","\n","\n","    if DTC:\n","        DTC_image = image.copy()\n","        _trackMap = Apply_trackmap(centroid_dict, _trackMap, colorPool, 3)\n","        DTC_image = cv2.add(e.convrt2Image(_trackMap), image) \n","        DTCShow = DTC_image\n","        for id, box in centroid_dict.items():\n","            center_bird = box[0], box[1]\n","            if not id in coupleZone:\n","                cv2.rectangle(DTCShow,(box[4], box[5]),(box[6], box[7]),(0,255,0),2)\n","                cv2.rectangle(DTCShow,(box[4], box[5]-13),(box[4]+len(str(id))*10, box[5]),(0,200,255),-1)\n","                cv2.putText(DTCShow,str(id),(box[4]+2, box[5]-2),cv2.FONT_HERSHEY_SIMPLEX,.4,(0,0,0),1,cv2.LINE_AA)\n","        for coupled in couples:\n","            p1 , p2 = coupled\n","            couplesID = couples[coupled]['id']\n","            couplesBox = couples[coupled]['box']\n","            cv2.rectangle(DTCShow, couplesBox[2:4], couplesBox[4:], (0,150,255), 4)\n","            loc = couplesBox[0] , couplesBox[3]\n","            offset = len(str(couplesID)*5)\n","            captionBox = (loc[0] - offset, loc[1]-13), (loc[0] + offset, loc[1])\n","            cv2.rectangle(DTCShow,captionBox[0],captionBox[1],(0,200,255),-1)\n","            wc = captionBox[1][0] - captionBox[0][0]\n","            hc = captionBox[1][1] - captionBox[0][1]\n","            cx = captionBox[0][0] + wc // 2\n","            cy = captionBox[0][1] + hc // 2\n","            textLoc = (cx - offset, cy + 4)\n","            cv2.putText(DTCShow, str(couplesID) ,(textLoc),cv2.FONT_HERSHEY_SIMPLEX,.4,(0,0,0),1,cv2.LINE_AA)\n","        DTCVid.write(DTCShow)\n","\n","\n","    if SocialDistance:\n","        SDimage, birdSDimage = Apply_ellipticBound(centroid_dict, image, calibration, redZone, greenZone, yellowZone, final_redZone, coupleZone, couples, CircleradiusForIndivsual, CircleradiusForCouples)\n","        SDimageVid.write(SDimage)\n","\n","\n","    cv2.waitKey(3)\n","print('::: Analysis Completed')\n","\n","cap.release()\n","if DTC: DTCVid.release(); print(\"::: Video Write Completed : \", Path_For_DTC)\n","if SocialDistance: SDimageVid.release() ; print(\"::: Video Write Completed : \", Path_For_SocialDistance)\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[{"file_id":"1lnt8tXIn03kUbtK0_f6nfRuPgw4TEChe","timestamp":1661565590730},{"file_id":"https://github.com/DrMahdiRezaei/DeepSOCIAL/blob/master/YOLOv4_DeepSOCIAL_LoadfromDrive-1.ipynb","timestamp":1661565555890}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
